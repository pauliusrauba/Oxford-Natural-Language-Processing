{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formative 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'stop_words' from 'sklearn.feature_extraction' (C:\\Users\\Paulius\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e7c683ebddb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'stop_words' from 'sklearn.feature_extraction' (C:\\Users\\Paulius\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.util import bigrams\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(list(tok.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to clean text\n",
    "def clean(text):\n",
    "    return [w.strip(punctuation) for w in text.strip().split() if w.strip(punctuation) != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('formative4_data/arxiv_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fastText embeddings\n",
    "with open('formative4_data/fasttext_vectors.p', 'rb') as f:\n",
    "    fasttext_vecs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean abstracts\n",
    "data['cleaned'] = data.abstract.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of labels\n",
    "labels = ['ap-2010', 'ap-2020', 'cs-2010', 'cs-2020']\n",
    "\n",
    "# Define dictionary for label look-up\n",
    "label2id = {'ap-2010': 0, 'ap-2020': 1, 'cs-2010': 2, 'cs-2020': 3}\n",
    "\n",
    "# Define dictionary for reverse label look-up\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Convert labels\n",
    "data['label'] = data.label.apply(lambda x: label2id[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, development, and test sets\n",
    "train, dev_test = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=0)\n",
    "dev, test = train_test_split(dev_test, test_size=0.5, stratify=dev_test['label'], random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Describing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) Create counters containing word frequencies for the four classes. Compare the 30 most frequent words for each class. What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data structures\n",
    "vocab_uni = defaultdict(Counter)\n",
    "\n",
    "# Create vocabularies\n",
    "for a, l in zip(train.cleaned, train.label):\n",
    "    vocab_uni[id2label[l]].update([w for w in a if w not in stop_words.ENGLISH_STOP_WORDS])\n",
    "\n",
    "# Inspect most frequent unigrams\n",
    "for l in labels:\n",
    "    print('30 most frequent unigrams in {} abstracts:'.format(l), vocab_uni[l].most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent words in ap-2010 and ap-2020 are very similar and can be roughly split into subject-specific words (_galaxies_, _mass_, _stars_, etc.) and scientific jargon (_observed_, _models_, _results_, etc.). The same holds for the most frequent words in cs-2010 and cs-2020, with some words being subject-specific (_algorithm_, _performance_, _network_, etc.) and some being terms typical for scientific writing (_problem_, _using_, _proposed_, etc.). In general, the lexical differences are much larger between subjects than between years. The temporal differences, on the other hand, seem to be larger for cs than for ap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) Compute the pairwise Jaccard similarities for the vocabularies of all four classes and display them\n",
    "in a four-by-four similarity matrix. What do you observe? Are the similarities in line with your expectations? Compare the numbers to the Jaccard similarities computed for male and female content words \n",
    "in formative 2. What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for calculating Jaccard similarity\n",
    "def jaccard_sim(vocab_1, vocab_2):\n",
    "    return len(vocab_1.intersection(vocab_2)) / len(vocab_1.union(vocab_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize similarity matrix\n",
    "sims_uni = defaultdict(Counter)\n",
    "\n",
    "# Fill similarity matrix\n",
    "for l_1 in labels:\n",
    "    for l_2 in labels:\n",
    "        sims_uni[l_1][l_2] = jaccard_sim(set(vocab_uni[l_1]), set(vocab_uni[l_2]))\n",
    "        \n",
    "# Display similarity matrix\n",
    "pd.DataFrame.from_dict(sims_uni, orient='index', columns=labels).reindex(index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jaccard similarity values are overall rather low. They are, e.g., much lower than the values obtained for different genders in formative 2. This suggests that the lexical differences should allow for some success in predicting the subject and year of abstracts from the abstract text. In addition, the similarity values are roughly 0.1 higher when comparing abstracts from the same subject as opposed to the same year, which is in line with the observation made in B). It is also interesting to notice that the temporal differences are slightly larger for cs than ap (0.29 versus 0.27)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) Repeat steps A) and B) for bigrams instead of unigrams, i.e., create bigram vocabularies for the four classes, compare the 30 most frequent bigrams, and examine the pairwise Jaccard similarities. Are the observed patterns similar to what you found for unigrams?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data structures\n",
    "vocab_bi = defaultdict(Counter)\n",
    "\n",
    "# Create vocabularies\n",
    "for a, l in zip(train.cleaned, train.label):\n",
    "    vocab_bi[id2label[l]].update(bigrams([w for w in a if w not in stop_words.ENGLISH_STOP_WORDS]))\n",
    "    \n",
    "# Inspect most frequent bigrams\n",
    "for l in labels:\n",
    "    print('30 most frequent bigrams in {} abstracts:'.format(l), vocab_bi[l].most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize similarity matrix\n",
    "sims_bi = defaultdict(Counter)\n",
    "\n",
    "# Fill similarity matrix\n",
    "for l_1 in labels:\n",
    "    for l_2 in labels:\n",
    "        sims_bi[l_1][l_2] = jaccard_sim(set(vocab_bi[l_1]), set(vocab_bi[l_2]))\n",
    "        \n",
    "# Display similarity matrix\n",
    "pd.DataFrame.from_dict(sims_bi, orient='index', columns=labels).reindex(index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for bigrams are overall similar to the results observed for unigrams. The most frequent bigrams can again be split into subject-specific phrases (_magnetic field_, _neural network_, etc.) and scientific jargon (_et al_, _experimental results_, etc.). The Jaccard similarity values are even lower than for unigrams. Interestingly, the trend that temporal differences are larger for cs than ap is much more pronounced for bigrams than for unigrams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) Now generate an embedding for each abstract by averaging the embeddings for individual words. As in subtasks A), B), and C) (and keeping in mind the results from formative 1), you should again remove stopwords for this \n",
    "analysis. Using t-SNE, reduce all abstract embeddings to two dimensions. Plot the resulting two-dimensional vectors colored by label (i.e., subject and year). What do you observe? Based on the plot, \n",
    "how would the confusion matrix for predicting the label from the average embedding probably look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to compute abstract embedding\n",
    "def abstract2vec(a, vecs):\n",
    "    return np.array([vecs[w] for w in a if w not in stop_words.ENGLISH_STOP_WORDS]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate abstract embeddings\n",
    "data['vec'] = data.cleaned.apply(lambda x: abstract2vec(x, fasttext_vecs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset of vectors for t-SNE\n",
    "tsne_vectors = np.array(list(data['vec']))\n",
    "\n",
    "# Train t-SNE\n",
    "tsne = TSNE(n_components=2, verbose=0, perplexity=50, n_iter=1000, random_state=0)\n",
    "tsne_results = tsne.fit_transform(tsne_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "\n",
    "for l in labels:\n",
    "    ax.scatter(tsne_results[data.label==label2id[l], 0], tsne_results[data.label==label2id[l], 1], label=l, alpha=0.25)\n",
    "ax.set_title('t-SNE plot of abstract embeddings')\n",
    "ax.set_xlabel('t-SNE dimension 1')\n",
    "ax.set_ylabel('t-SNE dimension 2')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings form two major clusters in embedding space that correspond to the two subjects. The temporal difference is visible in embedding space to a lesser degree, even though the center of the embeddings for cs-2020 seems to have shifted compared to the embeddings for cs-2010. Overall, this suggest that predicting subject differences might be easier than temporal differences based on neural representation. However, it is important to notice that the models used in this formative do not simply average word embeddings, so the observations should be seen as a lower bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class LSTMDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, w2id):\n",
    "        \n",
    "        # Encode abstracts\n",
    "        self.abstracts = list(data.cleaned.apply(lambda x: [w2id[w] if w in w2id else 1 for w in x]))\n",
    "        \n",
    "        # Store labels\n",
    "        self.labels = list(data.label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        abstract = self.abstracts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return abstract, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collate function\n",
    "def lstm_collate(batch):\n",
    "    \n",
    "    # Store batch size\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # Separate abstracts and labels\n",
    "    abstracts = [a for a, _ in batch]\n",
    "    labels = torch.tensor([l for _, l in batch]).long()\n",
    "\n",
    "    # Store length of longest abstract in batch\n",
    "    max_len = max(len(a) for a in abstracts)\n",
    "\n",
    "    # Create padded abstract tensors\n",
    "    abstracts_pad = torch.zeros((batch_size, max_len)).long()\n",
    "    for i, a in enumerate(abstracts):\n",
    "        abstracts_pad[i, :len(a)] = torch.tensor(a)\n",
    "\n",
    "    return abstracts_pad, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM classifier\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, pretrained_emb, hidden_dim, output_dim):\n",
    "        \n",
    "        # Define network layers\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_emb, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(pretrained_emb.shape[1], hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Define dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, abstracts):\n",
    "            \n",
    "        # Define flow of tensors through network\n",
    "        emb = self.embedding(abstracts)\n",
    "        output, (hidden, cell) = self.lstm(self.dropout(emb))\n",
    "        return self.linear(self.dropout(output[:, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary for word look-up\n",
    "word_counter = Counter([w for a in train.cleaned for w in a])\n",
    "w2id = {w: i + 2 for i, w in enumerate(w for w, c in word_counter.most_common())}\n",
    "\n",
    "# Create dictionary for reverse word look-up\n",
    "id2w = {i: w for w, i in w2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = LSTMDataset(train, w2id)\n",
    "dev_dataset = LSTMDataset(dev, w2id)\n",
    "test_dataset = LSTMDataset(test, w2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, collate_fn=lstm_collate, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=100, collate_fn=lstm_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, collate_fn=lstm_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of pretrained embeddings\n",
    "fasttext_emb = torch.tensor([fasttext_vecs[id2w[i]] for i in range(2, len(id2w) + 2)]).float()\n",
    "fasttext_emb = torch.cat((torch.zeros((1, 300)), fasttext_emb.mean(axis=0, keepdim=True), fasttext_emb), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LSTMClassifier(fasttext_emb, 200, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and training objective\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device and move model to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "for e in range(1, 16):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, b in enumerate(train_loader):\n",
    "        \n",
    "        # Perform forward pass\n",
    "        optimizer.zero_grad()\n",
    "        abstracts, lbls = [t.to(device) for t in b]\n",
    "        output = model(abstracts)\n",
    "        loss = criterion(output, lbls)\n",
    "        \n",
    "        # Perform backpropagation and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate model on development data\n",
    "    model.eval()\n",
    "\n",
    "    y_true = list()\n",
    "    y_pred = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b in dev_loader:\n",
    "            abstracts, lbls = [t.to(device) for t in b]\n",
    "            output = model(abstracts)\n",
    "            max_output = output.argmax(dim=1)\n",
    "            y_true.extend(lbls.tolist())\n",
    "            y_pred.extend(max_output.tolist())\n",
    "            \n",
    "    print('Development accuracy after {} epoch(s): {:.2f}'.format(e, accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "model.eval()\n",
    "\n",
    "y_true = list()\n",
    "y_pred = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in test_loader:\n",
    "        abstracts, lbls = [t.to(device) for t in b]\n",
    "        output = model(abstracts)\n",
    "        max_output = output.argmax(dim=1)\n",
    "        y_true.extend(lbls.tolist())\n",
    "        y_pred.extend(max_output.tolist())\n",
    "\n",
    "print('Test accuracy: {:.2f}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) What test accuracy does the LSTM classifier achieve? How does this compare to the performance of a random baseline?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM achieves a test accuracy of 0.64. This is way above the random baseline of 0.25. Notice that due to the small dataset size, results for the LSTM can vary drastically for different training runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) Create a confusion matrix of the true versus predicted labels on the test set. What do you observe? How do the results relate to the expactations developed in Part I?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize confusion matrix\n",
    "c_matrix = defaultdict(Counter)\n",
    "\n",
    "# Fill confusion matrix\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    c_matrix[id2label[t]][id2label[p]] += 1\n",
    "\n",
    "# Display confusion matrix\n",
    "pd.DataFrame.from_dict(c_matrix, orient='index', columns=labels).reindex(index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is strinking to see that there are very little misclassifications along the subject axis. Out of all 600 cs abstracts, e.g., only 2 are classified as ap abstracts. Also, misclassifications along the time axis are more common for ap than cs abstracts, which is in line with our predictions made in Part I (see, e.g., the t-SNE plot). For ap, the model has a pronounced tendency to overpredict ap-2020, i.e., it seems to have failed to learn a distinction between ap-2010 and ap-2020 altogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: BERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class BERTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tok = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # Truncate and encode abstracts\n",
    "        self.abstracts = list(data.abstract.apply(self.tok.encode, truncation_strategy='longest_first', max_length=100))\n",
    "        \n",
    "        # Store labels\n",
    "        self.labels = list(data.label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        abstract = self.abstracts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return abstract, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define collate function\n",
    "def bert_collate(batch):\n",
    "    \n",
    "    # Store batch size\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # Separate abstracts and labels\n",
    "    abstracts = [a for a, _ in batch]\n",
    "    labels = torch.tensor([l for _, l in batch]).long()\n",
    "    \n",
    "    # Store length of longest abstract in batch\n",
    "    max_len = max(len(a) for a in abstracts)\n",
    "    \n",
    "    # Create padded abstract, mask, and segment tensors\n",
    "    abstracts_pad = torch.zeros((batch_size, max_len)).long()\n",
    "    masks_pad = torch.zeros((batch_size, max_len)).long()\n",
    "    segs_pad = torch.zeros((batch_size, max_len)).long()\n",
    "    for i, a in enumerate(abstracts):\n",
    "        abstracts_pad[i, :len(a)] = torch.tensor(a)\n",
    "        masks_pad[i, :len(a)] = 1\n",
    "    \n",
    "    return abstracts_pad, masks_pad, segs_pad, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BERT classifier\n",
    "class BERTClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Define network layers\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.linear = nn.Linear(768, 4)\n",
    "        \n",
    "        # Define dropout\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Freeze BERT layers\n",
    "        for n, p in self.bert.named_parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, abstracts, masks, segs):\n",
    "        \n",
    "        # Define flow of tensors through network\n",
    "        output_bert = self.bert(abstracts, attention_mask=masks, token_type_ids=segs)[0].mean(axis=1)\n",
    "        return self.linear(self.dropout(output_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = BERTDataset(train)\n",
    "dev_dataset = BERTDataset(dev)\n",
    "test_dataset = BERTDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, collate_fn=bert_collate, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=100, collate_fn=bert_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, collate_fn=bert_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = BERTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and training objective\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device and move model to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "for e in range(1, 6):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, b in enumerate(train_loader):\n",
    "\n",
    "        # Perform forward pass\n",
    "        optimizer.zero_grad()\n",
    "        abstracts, masks, segs, lbls = [t.to(device) for t in b]\n",
    "        output = model(abstracts, masks, segs)\n",
    "        loss = criterion(output, lbls)\n",
    "        \n",
    "        # Perform backpropagation and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "  \n",
    "    # Evaluate model on development data\n",
    "    model.eval()\n",
    "\n",
    "    y_true = list()\n",
    "    y_pred = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b in dev_loader:\n",
    "            abstracts, masks, segs, lbls = [t.to(device) for t in b]\n",
    "            output = model(abstracts, masks, segs)\n",
    "            max_output = output.argmax(dim=1)\n",
    "            y_true.extend(lbls.tolist())\n",
    "            y_pred.extend(max_output.tolist())\n",
    "            \n",
    "    print('Accuracy after {} epoch(s): {:.2f}'.format(e, accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "model.eval()\n",
    "\n",
    "y_true = list()\n",
    "y_pred = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in test_loader:\n",
    "        abstracts, masks, segs, lbls = [t.to(device) for t in b]\n",
    "        output = model(abstracts, masks, segs)\n",
    "        max_output = output.argmax(dim=1)\n",
    "        y_true.extend(lbls.tolist())\n",
    "        y_pred.extend(max_output.tolist())\n",
    "\n",
    "print('Test accuracy: {:.2f}'.format(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A) What test accuracy does the BERT classifier achieve? How does this compare to the performance of the LSTM classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT achieves a test accuracy of 0.74, which is substantially better than the already well-performing LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize confusion matrix\n",
    "c_matrix = defaultdict(Counter)\n",
    "\n",
    "# Fill confusion matrix\n",
    "for t, p in zip(y_true, y_pred):\n",
    "    c_matrix[id2label[t]][id2label[p]] += 1\n",
    "\n",
    "# Display confusion matrix\n",
    "pd.DataFrame.from_dict(c_matrix, orient='index', columns=labels).reindex(index=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) Create a confusion matrix of the true versus predicted labels on the test set. What do you observe? How do the results relate to the expactations developed in Part I? How do they compare to the confusion matrix for the LSTM classifier?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are almost no misclassifications along the subject axis, even less than for the LSTM. Misclassifications along the time axis are again more common for ap than cs abstracts, in accordance with our predictions made in Part I (see, e.g., the t-SNE plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) Based on the confusion matrix (and taking into account the results from Part II), what conclusions can you draw about the speed with which astrophysics and computer science changed as scientific fields between 2010 and 2020?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All results in this formative, i.e., the descriptive analyses in Part I, the results of the LSTM in Part II, and the results of BERT in Part III, suggest that the differences between abstracts from 2010 and 2020 are larger for computer science than astrophysics. This indicates that computer science has been changing faster than astrophysics. Looking at the number of abstracts in the full arXiv dataset, we can see that computer science has also experienced a substantial growth between 2010 and 2020. It is hence an interesting question whether the change in computer science is due to a shift or expansion of studied topics. The t-SNE seems to suggest the latter, i.e., the dominant region of cs-2010 abstracts encompasses part of the cs-2020 abstracts, but there is also a region of cs-2020 abstracts without cs-2010 abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D) Make predictions about possible advantages and disadvantages of using a model based on n-grams and smoothing (e.g., Naive Bayes) for this task, as opposed to neural models. Your discussion should touch upon the \n",
    "unigram and bigram statistics reported in Part I.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many scientific concepts have names consisting of more than one word (e.g., _magnetic field_,  _neural network_, etc.). As we have seen in Part I, such bigrams are much more discriminative for subject and year than unigrams, and hence having qualitative representations for them is crucial for the task. In this context, neural models have the advantage that it does not matter whether they have seen a specific bigram in training as long as they have an embedding for both words in the bigram, i.e., the need for smoothing does not occur. This is possible since neural models learn compositional patterns that allow them to combine arbitrary sequences of words into meaningful representations. A Naive Bayes model, on the other hand, would need to completely discard unseen bigrams or use smoothed scores based on, e.g., the unigram scores (in back-off models), which by definition cannot capture compositional effects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
